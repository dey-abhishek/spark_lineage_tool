#!/bin/bash
"""Script to generate comprehensive mock files for testing."""

# This script serves as documentation and can be extended to generate actual files

# Scala examples - patterns similar to PySpark
# 01_SimpleJob.scala - read.parquet, write.parquet
# 02_ComplexPipeline.scala - multiple joins, aggregations
# 03_StringInterpolation.scala - s"path/${var}"
# 04_CustomWrappers.scala - org-specific IO methods
# 05-15: Additional Scala patterns

# Hive SQL examples
# 01_insert_overwrite.hql - INSERT OVERWRITE TABLE
# 02_ctas.hql - CREATE TABLE AS SELECT
# 03_dynamic_partitions.hql - Dynamic partition writes
# 04_multi_insert.hql - FROM...INSERT pattern
# 05-15: Complex queries, views, ALTER statements

# Shell scripts
# 01_hdfs_copy.sh - hdfs dfs -cp
# 02_distcp_job.sh - hadoop distcp
# 03_spark_submit.sh - spark-submit with args
# 04_hive_batch.sh - hive -f/-e
# 05-15: Mixed operations, loops, conditional logic

# NiFi flows - JSON format
# 01_flow_hdfs_to_hive.json - GetHDFS -> PutHiveQL
# 02_flow_with_params.json - Parameter contexts
# 03-10: Various processor combinations

# Config files
# job.properties, application.yaml, spark-defaults.conf
# 01-10: Various config formats with references

echo "Mock file templates documented"
echo "Actual files should be generated based on these patterns"

